{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.4.1-cp38-cp38-win_amd64.whl (199.4 MB)\n",
      "Requirement already satisfied: networkx in c:\\users\\lakie\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\lakie\\anaconda3\\lib\\site-packages (from torch) (3.0.12)\n",
      "Requirement already satisfied: fsspec in c:\\users\\lakie\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\lakie\\anaconda3\\lib\\site-packages (from torch) (1.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lakie\\anaconda3\\lib\\site-packages (from torch) (2.11.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\lakie\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\lakie\\anaconda3\\lib\\site-packages (from sympy->torch) (1.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\lakie\\anaconda3\\lib\\site-packages (from jinja2->torch) (1.1.1)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-2.4.1\n"
     ]
    }
   ],
   "source": [
    "! pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here:\n",
    "\n",
    "    We define sequences, a tensor containing sequences of numbers, and targets, which are the next numbers in each sequence. This is a basic sequence prediction task.\n",
    "\n",
    "    sequences.unsqueeze(-1) adds a feature dimension. This makes each number in the sequence a feature vector, even though it’s just a scalar (dimension = 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape :  torch.Size([3, 4])\n",
      "Input shape :  torch.Size([3, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Sample input: Sequence of numbers\n",
    "# Task: Predict the next number in the sequence\n",
    "\n",
    "# Let's use a synthetic dataset: sequences of numbers where the task is to predict the next number\n",
    "sequences = torch.tensor([[1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6]], dtype=torch.float32)\n",
    "targets = torch.tensor([5, 6, 7], dtype=torch.float32)\n",
    "\n",
    "\n",
    "print(\"Input shape : \",sequences.shape) \n",
    "\n",
    "#many sequence models, such as RNNs, LSTMs, or transformers, expect the input to have a 3D shape: \n",
    "#(batch_size, sequence_length, input_features). The input_features dimension is required because \n",
    "#these models often expect each token in the sequence to have a feature vector.\n",
    "\n",
    "sequences = sequences.unsqueeze(-1)  # Add a feature dimension for input\n",
    "print(\"Input shape : \",sequences.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 37.076839447021484\n",
      "Predictions: [0.1150898  0.11644439 0.11835956]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 20, Loss: 13.387279510498047\n",
      "Predictions: [2.5198996 2.5413985 2.5526867]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 40, Loss: 3.3877270221710205\n",
      "Predictions: [4.424144  4.4299173 4.4332113]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 60, Loss: 0.8467133045196533\n",
      "Predictions: [5.6048865 5.6096697 5.6123714]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 80, Loss: 0.6623954176902771\n",
      "Predictions: [6.0319357 6.036786  6.0395117]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 100, Loss: 0.665977418422699\n",
      "Predictions: [6.0610423 6.066118  6.0689626]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 120, Loss: 0.6614015698432922\n",
      "Predictions: [6.0105906 6.01597   6.0189743]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 140, Loss: 0.6607268452644348\n",
      "Predictions: [5.9903936 5.996175  5.999388 ]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 160, Loss: 0.6601809859275818\n",
      "Predictions: [5.991784  5.9981093 6.0015993]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 180, Loss: 0.6593821048736572\n",
      "Predictions: [5.9939537 6.001099  6.0049887]\n",
      "True Values: [5. 6. 7.]\n"
     ]
    }
   ],
   "source": [
    "# __init__ function:\n",
    "# self.rnn = nn.RNN(input_size, hidden_size, batch_first=True) creates an RNN layer. \n",
    "# The input_size is the number of features (1 in this case, since each number in the sequence is a scalar), \n",
    "# the hidden_size defines the size of the hidden state. \n",
    "# batch_first=True ensures that the input shape is (batch_size, sequence_length, input_size), making it more intuitive to work with.\n",
    "# self.fc = nn.Linear(hidden_size, output_size) defines a fully connected (linear) layer that takes the RNN's hidden state output and maps it to the desired output (here, predicting the next number).\n",
    "\n",
    "# forward function:\n",
    "# The input x is passed through the RNN layer, which returns two values: out (the output at each time step) and hidden (the hidden state at the final time step). For simplicity, we don’t use the hidden state here, but it can be useful for more advanced tasks.\n",
    "# out[:, -1, :] extracts the output from the last time step (we are interested in the prediction for the last token in the sequence).\n",
    "# Finally, this output is passed through the fully connected layer (self.fc) to get the predicted next number.\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, hidden = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])  # Take the last time step's output\n",
    "        return out\n",
    "\n",
    "    \n",
    "\n",
    "# Model instantiation\n",
    "\n",
    "# input_size=1 because each element in the sequence is a scalar.\n",
    "# hidden_size=10 is the size of the hidden state, a tunable parameter that controls the capacity of the model.\n",
    "# output_size=1 because the model predicts a single number (the next number in the sequence).\n",
    "rnn_model = RNNModel(input_size=1, hidden_size=10, output_size=1)\n",
    "\n",
    "# sets up the optimizer (Adam), which updates the model's parameters during training to minimize the loss.\n",
    "optimizer = optim.Adam(rnn_model.parameters(), lr=0.01)\n",
    "\n",
    "#  We use Mean Squared Error (MSE) since this is a regression task (predicting a number).    \n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop with prediction display\n",
    "for epoch in range(200):\n",
    "    rnn_model.train()  # Set model to training mode\n",
    "    optimizer.zero_grad()  # Clear gradients\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = rnn_model(sequences)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = criterion(outputs, targets.unsqueeze(1))\n",
    "    \n",
    "    # Backpropagation and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Every 20 epochs, print loss and predictions\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "        \n",
    "        # Switch model to evaluation mode for prediction\n",
    "        rnn_model.eval()\n",
    "        with torch.no_grad():  # Disable gradient calculations for predictions\n",
    "            predictions = rnn_model(sequences)\n",
    "            print(f\"Predictions: {predictions.squeeze().numpy()}\")\n",
    "            print(f\"True Values: {targets.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 37.32528305053711\n",
      "Predictions: [0.06150491 0.05338822 0.05241326]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 20, Loss: 15.670123100280762\n",
      "Predictions: [2.1711187 2.2454102 2.2832065]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 40, Loss: 2.666585922241211\n",
      "Predictions: [4.686811  4.7192626 4.7290587]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 60, Loss: 0.6852652430534363\n",
      "Predictions: [6.2105045 6.233781  6.2452507]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 80, Loss: 0.6673634052276611\n",
      "Predictions: [6.115544  6.137893  6.1488557]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 100, Loss: 0.6457936763763428\n",
      "Predictions: [5.9306   5.95434  5.965931]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 120, Loss: 0.6410209536552429\n",
      "Predictions: [5.9748173 6.0011587 6.013957 ]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 140, Loss: 0.6374949812889099\n",
      "Predictions: [5.982035  6.0121717 6.026728 ]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 160, Loss: 0.6318579316139221\n",
      "Predictions: [5.9666634 6.002833  6.0201893]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 180, Loss: 0.6214486956596375\n",
      "Predictions: [5.956663  6.0042744 6.026974 ]\n",
      "True Values: [5. 6. 7.]\n"
     ]
    }
   ],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, (hidden, cell) = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])  # Last time step\n",
    "        return out\n",
    "\n",
    "# Model instantiation\n",
    "lstm_model = LSTMModel(input_size=1, hidden_size=10, output_size=1)\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop with prediction display\n",
    "for epoch in range(200):\n",
    "    lstm_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = lstm_model(sequences)\n",
    "    loss = criterion(outputs, targets.unsqueeze(1))\n",
    "    \n",
    "    # Backpropagation and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Every 20 epochs, print loss and predictions\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "        \n",
    "        # Prediction step\n",
    "        lstm_model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = lstm_model(sequences)\n",
    "            print(f\"Predictions: {predictions.squeeze().numpy()}\")\n",
    "            print(f\"True Values: {targets.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 36.85272216796875\n",
      "Predictions: [0.07471006 0.08703961 0.097192  ]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 20, Loss: 11.1943998336792\n",
      "Predictions: [2.8022923 2.9271617 3.0100799]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 40, Loss: 0.4933995306491852\n",
      "Predictions: [5.997852  6.166354  6.2835183]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 60, Loss: 0.577017068862915\n",
      "Predictions: [6.1009264 6.2671776 6.3828936]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 80, Loss: 0.505304217338562\n",
      "Predictions: [5.7356434 5.9035044 6.0209093]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 100, Loss: 0.4696296751499176\n",
      "Predictions: [5.87276   6.0650563 6.200328 ]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 120, Loss: 0.4311773478984833\n",
      "Predictions: [5.7507224 5.98088   6.1493607]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 140, Loss: 0.35201308131217957\n",
      "Predictions: [5.66101   5.9833117 6.2254677]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 160, Loss: 0.21636372804641724\n",
      "Predictions: [5.507787  6.003061  6.3923345]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 180, Loss: 0.08031509071588516\n",
      "Predictions: [5.267324  6.025836  6.6091228]\n",
      "True Values: [5. 6. 7.]\n"
     ]
    }
   ],
   "source": [
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        self.bilstm = nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)  # BiLSTM has 2x hidden size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.bilstm(x)\n",
    "        out = self.fc(out[:, -1, :])  # Last time step\n",
    "        return out\n",
    "\n",
    "# Model instantiation\n",
    "bilstm_model = BiLSTMModel(input_size=1, hidden_size=10, output_size=1)\n",
    "optimizer = optim.Adam(bilstm_model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop with prediction display\n",
    "for epoch in range(200):\n",
    "    bilstm_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = bilstm_model(sequences)\n",
    "    loss = criterion(outputs, targets.unsqueeze(1))\n",
    "    \n",
    "    # Backpropagation and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Every 20 epochs, print loss and predictions\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "        \n",
    "        # Prediction step\n",
    "        bilstm_model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = bilstm_model(sequences)\n",
    "            print(f\"Predictions: {predictions.squeeze().numpy()}\")\n",
    "            print(f\"True Values: {targets.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 35.594730377197266\n",
      "Predictions: [0.1967281  0.17286184 0.14373879]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 20, Loss: 14.406082153320312\n",
      "Predictions: [2.4157283 2.444249  2.4600184]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 40, Loss: 1.5141925811767578\n",
      "Predictions: [5.168911  5.1777763 5.1827106]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 60, Loss: 0.7221739292144775\n",
      "Predictions: [6.2545414 6.2597113 6.262142 ]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 80, Loss: 0.6752507090568542\n",
      "Predictions: [6.097909  6.1026077 6.104924 ]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 100, Loss: 0.6631235480308533\n",
      "Predictions: [5.9584937 5.9633665 5.9658446]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 120, Loss: 0.6614049077033997\n",
      "Predictions: [5.9885497 5.9937882 5.996615 ]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 140, Loss: 0.6608798503875732\n",
      "Predictions: [6.0018063 6.007462  6.0106163]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 160, Loss: 0.6602396368980408\n",
      "Predictions: [5.994605  6.0007887 6.004321 ]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 180, Loss: 0.6594054698944092\n",
      "Predictions: [5.9944553 6.001385  6.0054545]\n",
      "True Values: [5. 6. 7.]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Sample input: Sequence of numbers\n",
    "# Task: Predict the next number in the sequence\n",
    "sequences = torch.tensor([[1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6]], dtype=torch.float32)\n",
    "targets = torch.tensor([5, 6, 7], dtype=torch.float32)\n",
    "sequences = sequences.unsqueeze(-1)  # Add a feature dimension for input\n",
    "\n",
    "# Encoder LSTM\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        outputs, (hidden, cell) = self.lstm(x)\n",
    "        return hidden, cell  # Return the final hidden and cell states\n",
    "\n",
    "# Decoder LSTM\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, hidden, cell):\n",
    "        outputs, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "        prediction = self.fc(outputs[:, -1, :])  # Take the output of the last time step\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 1  # Since the input is just a single number per step\n",
    "hidden_size = 10  # Size of the hidden state\n",
    "output_size = 1   # Single value output\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Instantiate models\n",
    "encoder = Encoder(input_size=input_size, hidden_size=hidden_size)\n",
    "decoder = Decoder(hidden_size=hidden_size, output_size=output_size)\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(200):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Encoder forward pass\n",
    "    encoder_hidden, encoder_cell = encoder(sequences)\n",
    "    \n",
    "    # Decoder initial input (Use the hidden state as input to the decoder)\n",
    "    # We'll use a zero tensor for the first input to the decoder (like a <start> token)\n",
    "    decoder_input = torch.zeros((sequences.size(0), 1, hidden_size))  # Shape: (batch_size, 1, hidden_size)\n",
    "    \n",
    "    # Decoder forward pass (start with the hidden and cell states from the encoder)\n",
    "    decoder_output, _, _ = decoder(decoder_input, encoder_hidden, encoder_cell)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(decoder_output, targets.unsqueeze(1))\n",
    "    \n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Every 20 epochs, print loss and predictions\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "        \n",
    "        # Evaluation mode for prediction\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        with torch.no_grad():\n",
    "            encoder_hidden, encoder_cell = encoder(sequences)\n",
    "            decoder_input = torch.zeros((sequences.size(0), 1, hidden_size))  # Initial decoder input\n",
    "            decoder_output, _, _ = decoder(decoder_input, encoder_hidden, encoder_cell)\n",
    "            print(f\"Predictions: {decoder_output.squeeze().numpy()}\")\n",
    "            print(f\"True Values: {targets.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 38.947322845458984\n",
      "Predictions: [-0.13270167 -0.10514794 -0.08190268]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 20, Loss: 11.377039909362793\n",
      "Predictions: [2.8632393 2.8837392 2.8905833]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 40, Loss: 1.543578028678894\n",
      "Predictions: [5.140058  5.1433935 5.144601 ]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 60, Loss: 0.6688856482505798\n",
      "Predictions: [6.0871305 6.0900364 6.091098 ]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 80, Loss: 0.6841444373130798\n",
      "Predictions: [6.1327615 6.135635  6.136684 ]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 100, Loss: 0.66417396068573\n",
      "Predictions: [6.0043592 6.007227  6.008275 ]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 120, Loss: 0.6643080115318298\n",
      "Predictions: [5.9811273 5.9840503 5.9851174]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 140, Loss: 0.6639390587806702\n",
      "Predictions: [5.997347  6.0003533 6.0014496]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 160, Loss: 0.6638632416725159\n",
      "Predictions: [5.999854  6.0029507 6.00408  ]\n",
      "True Values: [5. 6. 7.]\n",
      "Epoch 180, Loss: 0.6637632846832275\n",
      "Predictions: [5.9974303 6.000632  6.0017977]\n",
      "True Values: [5. 6. 7.]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Sample input: Sequence of numbers\n",
    "# Task: Predict the next number in the sequence\n",
    "sequences = torch.tensor([[1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6]], dtype=torch.float32)\n",
    "targets = torch.tensor([5, 6, 7], dtype=torch.float32)\n",
    "sequences = sequences.unsqueeze(-1)  # Add a feature dimension for input\n",
    "\n",
    "# Encoder LSTM\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoder_outputs, (hidden, cell) = self.lstm(x)\n",
    "        return encoder_outputs, hidden, cell  # Return all encoder outputs, hidden and cell states\n",
    "\n",
    "# Attention Mechanism\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)  # Linear layer for attention weights\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))  # Context vector v\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # Repeat the hidden state across the sequence length to compute attention scores\n",
    "        hidden = hidden[-1].unsqueeze(1).repeat(1, encoder_outputs.size(1), 1)  # Shape: (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        # Calculate energy scores (similarity between hidden state and encoder outputs)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  # Shape: (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attention = torch.sum(self.v * energy, dim=2)  # Shape: (batch_size, seq_len)\n",
    "        attention_weights = torch.softmax(attention, dim=1)  # Normalize with softmax\n",
    "        \n",
    "        # Compute context vector (weighted sum of encoder outputs)\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)  # Shape: (batch_size, 1, hidden_size)\n",
    "        return context, attention_weights\n",
    "\n",
    "# Decoder LSTM with Attention\n",
    "class DecoderWithAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "        self.lstm = nn.LSTM(hidden_size * 2, hidden_size, batch_first=True)  # Input is concatenation of context + decoder input\n",
    "        self.attention = Attention(hidden_size)  # Attention mechanism\n",
    "        self.fc = nn.Linear(hidden_size, output_size)  # Fully connected layer for predictions\n",
    "    \n",
    "    def forward(self, x, hidden, cell, encoder_outputs):\n",
    "        # Calculate the context vector using the attention mechanism\n",
    "        context, attention_weights = self.attention(hidden, encoder_outputs)\n",
    "        \n",
    "        # Concatenate the context vector and the input to the decoder\n",
    "        lstm_input = torch.cat((x, context), dim=2)  # Shape: (batch_size, 1, hidden_size * 2)\n",
    "        \n",
    "        # Pass through the LSTM\n",
    "        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
    "        \n",
    "        # Generate the final prediction\n",
    "        prediction = self.fc(output[:, -1, :])  # Shape: (batch_size, output_size)\n",
    "        return prediction, hidden, cell, attention_weights\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 1  # Since the input is just a single number per step\n",
    "hidden_size = 10  # Size of the hidden state\n",
    "output_size = 1   # Single value output\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Instantiate models\n",
    "encoder = Encoder(input_size=input_size, hidden_size=hidden_size)\n",
    "decoder = DecoderWithAttention(hidden_size=hidden_size, output_size=output_size)\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(200):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Encoder forward pass\n",
    "    encoder_outputs, encoder_hidden, encoder_cell = encoder(sequences)\n",
    "    \n",
    "    # Initial decoder input (a zero tensor)\n",
    "    decoder_input = torch.zeros((sequences.size(0), 1, hidden_size))  # Shape: (batch_size, 1, hidden_size)\n",
    "    \n",
    "    # Decoder forward pass with attention\n",
    "    decoder_output, _, _, _ = decoder(decoder_input, encoder_hidden, encoder_cell, encoder_outputs)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(decoder_output, targets.unsqueeze(1))\n",
    "    \n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Every 20 epochs, print loss and predictions\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "        \n",
    "        # Evaluation mode for prediction\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        with torch.no_grad():\n",
    "            encoder_outputs, encoder_hidden, encoder_cell = encoder(sequences)\n",
    "            decoder_input = torch.zeros((sequences.size(0), 1, hidden_size))  # Initial decoder input\n",
    "            decoder_output, _, _, _ = decoder(decoder_input, encoder_hidden, encoder_cell, encoder_outputs)\n",
    "            print(f\"Predictions: {decoder_output.squeeze().numpy()}\")\n",
    "            print(f\"True Values: {targets.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
